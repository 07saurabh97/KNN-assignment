{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the KNN algorithm?"
      ],
      "metadata": {
        "id": "aSdJnSAQkKgS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfIzlavTkGHF"
      },
      "outputs": [],
      "source": [
        "'''\n",
        " The k-nearest neighbors (KNN) algorithm is a non-parametric, supervised learning classifier, which uses\n",
        " proximity to make classifications or predictions about the grouping of an individual data point.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How do you choose the value of K in KNN?"
      ],
      "metadata": {
        "id": "DpvzDvAOkOyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input data-\n",
        "The type of data you're using will determine the best value of k. For example, if the data has a lot of outliers\n",
        "or noise, you might want to choose a higher value of k.\n",
        "\n",
        "Odd or even-\n",
        "It's generally recommended to choose an odd value of k to avoid ties in classification. For example, if you have\n",
        "7 nearest neighbors, and 4 belong to class 2 and 3 belong to class 1, the point is confidently classified as\n",
        "belonging to class 2.\n",
        "\n",
        "Square root of n-\n",
        "A common rule of thumb is to choose the square root of the total number of data points (n).\n",
        "\n",
        "Cross-validation-\n",
        "You can use cross-validation techniques to help you find the best value of k for your dataset.\n",
        "\n",
        "Underfitting-\n",
        "If you choose a value of k that's too large, your model might become too simplistic and fail to capture the\n",
        "underlying patterns in the data. This is known as underfitting.\n",
        "\n",
        "\n",
        "The KNN algorithm is versatile and can be used for both classification and regression tasks. It's also robust to\n",
        "noisy training data and effective for non-linear relationships.\n",
        "'''"
      ],
      "metadata": {
        "id": "qtPGgiK2kQxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is the difference between KNN classifier and KNN regressor?"
      ],
      "metadata": {
        "id": "lhYnxG0skSM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A KNN classifier is used for classification tasks, meaning it predicts discrete categories like \"cat\" or \"dog\",\n",
        "while a KNN regressor is used for regression tasks, predicting continuous values like price or temperature, both\n",
        "using the same underlying KNN algorithm but with a different approach to making predictions: a KNN classifier\n",
        "determines the most frequent class among the nearest neighbors, while a KNN regressor calculates the average\n",
        "of the target values of the nearest neighbors to make a prediction\n",
        "'''"
      ],
      "metadata": {
        "id": "gpQaIpL8kUNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How do you measure the performance of KNN?"
      ],
      "metadata": {
        "id": "km0I-wn0kVTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The performance of a KNN (K-Nearest Neighbors) algorithm is typically measured using accuracy, which is calculated\n",
        "by comparing the predictions made by the model on a test dataset to the actual known labels, essentially counting\n",
        "the percentage of correct classifications; this is often done by splitting the data into training and testing sets,\n",
        "then evaluating the model on the test set using metrics like accuracy, precision, recall, and F1-score depending\n",
        "on the problem type.\n",
        "\n",
        "Key points about measuring KNN performance:\n",
        "\n",
        "Accuracy as primary metric:\n",
        "For simple classification tasks, accuracy is the most common metric used to evaluate KNN performance.\n",
        "\n",
        "Choosing the right K value:\n",
        "A crucial aspect of KNN is selecting the optimal value of \"k\" (the number of nearest neighbors to consider),\n",
        "which significantly impacts accuracy.\n",
        "\n",
        "Cross-validation:\n",
        "To find the best \"k\" value, cross-validation is often employed, where the data is split into multiple folds, and\n",
        "the model is trained and evaluated on each fold, then the results are averaged to get a more robust evaluation.\n",
        "'''"
      ],
      "metadata": {
        "id": "ngQSyP6vkXCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the curse of dimensionality in KNN?"
      ],
      "metadata": {
        "id": "OeScHFQkkXf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The curse of dimensionality in the context of k-Nearest Neighbors (kNN) refers to the challenges and problems\n",
        "that arise when the number of features (dimensions) in the dataset increases. It impacts the effectiveness and\n",
        "efficiency of kNN\n",
        "\n",
        "Distance between points: As the number of dimensions increases, the distance between any two data points becomes\n",
        "more similar and less meaningful.\n",
        "\n",
        "Nearest neighbor calculations: In high-dimensional spaces, the distances between nearest and farthest points from\n",
        "query points become almost equal. This makes it difficult for nearest neighbor calculations to discriminate candidate\n",
        "points.\n",
        "\n",
        "Overfitting: KNN is susceptible to overfitting due to the curse of dimensionality.\n",
        "\n",
        "Reduced sample size: In higher dimensions, there is effectively a reduction in sample size.\n",
        "'''"
      ],
      "metadata": {
        "id": "F_LqZ04SkbQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How do you handle missing values in KNN?"
      ],
      "metadata": {
        "id": "eSdFnqwJkcpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Handling missing values in k-Nearest Neighbors (kNN) is an essential preprocessing step to ensure the algorithm\n",
        "performs well. kNN can impute missing values using its inherent properties\n",
        "\n",
        "kNN-Based Imputation\n",
        "\n",
        "The most common approach is to use kNN imputation. It fills in missing values by looking at the values of the\n",
        "k-nearest neighbors based on a similarity measure (e.g., Euclidean distance, Manhattan distance).\n",
        "\n",
        "Steps for kNN Imputation:\n",
        "\n",
        "1. Choose k (number of neighbors): Select an appropriate value for k, typically based on cross-validation or domain\n",
        "knowledge.\n",
        "\n",
        "2. Calculate distances: For the record with missing values, calculate the distances to other records using only the\n",
        "non-missing features.\n",
        "\n",
        "3. Find k-nearest neighbors: Identify the k most similar records.\n",
        "\n",
        "4. Impute the missing value:\n",
        "\n",
        "   For numerical data: Take the mean (or median) of the corresponding values from the k-nearest neighbors.\n",
        "\n",
        "   For categorical data: Take the mode (most frequent value) of the corresponding values from the k-nearest neighbors.\n",
        "'''"
      ],
      "metadata": {
        "id": "6xxn1Z1rkeyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset with missing values\n",
        "data = np.array([[1, 2, np.nan],\n",
        "                 [3, np.nan, 5],\n",
        "                 [np.nan, 4, 6],\n",
        "                 [7, 8, 9]])\n",
        "\n",
        "# Create and fit the KNN imputer\n",
        "imputer = KNNImputer(n_neighbors=3, weights=\"uniform\")\n",
        "imputed_data = imputer.fit_transform(data)\n",
        "\n",
        "print(imputed_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PjFm-pjuxDXt",
        "outputId": "f0052724-059d-47d3-a0dc-40ab2b8e3897"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         2.         6.66666667]\n",
            " [3.         4.66666667 5.        ]\n",
            " [3.66666667 4.         6.        ]\n",
            " [7.         8.         9.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
        "which type of problem?"
      ],
      "metadata": {
        "id": "EUWYIa0XkgVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A KNN classifier is better suited for classification problems where the target variable is categorical (like\n",
        "\"yes/no\" or \"cat/dog\"), while a KNN regressor is better for regression problems where the target variable is\n",
        "continuous (like predicting house price or temperature) because the classifier predicts the most frequent class\n",
        "among the nearest neighbors, while the regressor calculates the average value of the nearest neighbors; essentially,\n",
        "the key difference is how they handle the output - a discrete class label for classification and a continuous value\n",
        "for regression\n",
        "'''"
      ],
      "metadata": {
        "id": "KEmEs2sokh_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
        "and how can these be addressed?"
      ],
      "metadata": {
        "id": "WT9YtEZRkjIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Strengths of kNN\n",
        "\n",
        "1. Simplicity:\n",
        "kNN is easy to understand and implement.\n",
        "It makes no assumptions about the underlying data distribution (non-parametric).\n",
        "\n",
        "2. Versatility:\n",
        "Can be applied to both classification and regression tasks.\n",
        "Handles multi-class classification naturally.\n",
        "\n",
        "3. Adaptable to Complex Boundaries:\n",
        "Can model complex decision boundaries if the data is well-labeled and sufficient.\n",
        "\n",
        "4. No Training Time:\n",
        "No explicit training phase (lazy learning). The entire dataset serves as the model, which can be advantageous when\n",
        "training time is a constraint.\n",
        "\n",
        "5. Robust to Small Datasets:\n",
        "Works well with small datasets, especially when the relationship between features is meaningful and distances are\n",
        "informative.\n",
        "\n",
        "\n",
        "Weaknesses of kNN\n",
        "\n",
        "1. Computational Cost:\n",
        "High memory usage and computational cost during prediction, as it involves calculating distances to all points in\n",
        "the dataset. Scales poorly with large datasets.\n",
        "\n",
        "2. Sensitive to Feature Scaling:\n",
        "Distance metrics like Euclidean distance are affected by the scale of features, which can skew results if features\n",
        "are not normalized.\n",
        "\n",
        "3. Sensitive to Irrelevant Features:\n",
        "Performance deteriorates when irrelevant or redundant features dominate the distance metric.\n",
        "\n",
        "4. Curse of Dimensionality:\n",
        "As the number of features increases, distances between points tend to converge, making it hard to differentiate\n",
        "between neighbors.\n",
        "\n",
        "5. Imbalanced Data:\n",
        "kNN can perform poorly on imbalanced datasets because the majority class can dominate the neighborhood.\n",
        "\n",
        "6. Choice of k:\n",
        "Results are highly sensitive to the choice of k. Too small a value can lead to overfitting, while too large a\n",
        "value may result in underfitting.\n",
        "\n",
        "7. Outlier Sensitivity:\n",
        "Outliers can significantly affect the neighborhood and skew predictions.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "soV3iOjhkkmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
      ],
      "metadata": {
        "id": "Rcthfsx4klr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Euclidean distance calculates the straight-line distance between two points, while Manhattan distance calculates\n",
        "the distance by summing the absolute differences along each axis, essentially following a grid-like path, often\n",
        "visualized as moving only on city streets (like a taxi) between two points; making Euclidean distance better for\n",
        "continuous data and Manhattan distance more suitable for data with grid-like structures or high-dimensional spaces.\n",
        "\n",
        "1. Euclidean Distance\n",
        "It measures the shortest straight-line distance (\"as the crow flies\") between two points.\n",
        "It is based on the Pythagorean theorem.\n",
        "\n",
        "2. Manhattan Distance\n",
        "It measures the distance between two points along axes at right angles (\"city block\" distance, like moving on a grid\n",
        "in a city).\n",
        "Adds up the absolute differences between corresponding dimensions.\n",
        "'''"
      ],
      "metadata": {
        "id": "oAydLB_ckneH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What is the role of feature scaling in KNN?"
      ],
      "metadata": {
        "id": "2wnjgEZnkoON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "In KNN (K-Nearest Neighbors), feature scaling plays a crucial role by ensuring that all features contribute equally\n",
        "to the distance calculations, preventing features with larger scales from dominating the decision-making process and\n",
        "leading to more accurate classifications, as KNN heavily relies on distance metrics between data points to make\n",
        "predictions; essentially, scaling ensures no single feature unfairly influences the distance calculation due to\n",
        "its magnitude alone.\n",
        "\n",
        "Distance-based algorithm:\n",
        "KNN is a distance-based algorithm, meaning it determines the nearest neighbors of a data point based on their\n",
        "distances in the feature space.\n",
        "\n",
        "Impact of feature scale:\n",
        "When features have different scales, the feature with a larger range will have a disproportionate impact on the\n",
        "distance calculation, potentially leading to inaccurate classifications.\n",
        "\n",
        "Importance of standardization:\n",
        "By scaling features to have a similar range (often using techniques like standardization or min-max scaling), all\n",
        "features contribute equally to the distance calculation, allowing KNN to make more informed decisions.\n",
        "'''"
      ],
      "metadata": {
        "id": "YXwm3QFhkp0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}